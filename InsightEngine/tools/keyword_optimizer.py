"""
å…³é”®è¯ä¼˜åŒ–ä¸­é—´ä»¶
ä½¿ç”¨ Qwen AI å°† Agent ç”Ÿæˆçš„æœç´¢è¯ä¼˜åŒ–ä¸ºæ›´é€‚åˆã€Œå°çº¢ä¹¦å“ç‰Œå†…å®¹/é€‰é¢˜åˆ†æã€åœºæ™¯çš„å…³é”®è¯
"""

from openai import OpenAI
import json
import sys
import os
from typing import List, Dict, Any
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ä»¥å¯¼å…¥config
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from config import settings
from loguru import logger

# æ·»åŠ utilsç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(os.path.dirname(current_dir))
utils_dir = os.path.join(root_dir, 'utils')
if utils_dir not in sys.path:
    sys.path.append(utils_dir)

from retry_helper import with_graceful_retry, SEARCH_API_RETRY_CONFIG

@dataclass
class KeywordOptimizationResponse:
    """å…³é”®è¯ä¼˜åŒ–å“åº”"""
    original_query: str
    optimized_keywords: List[str]
    reasoning: str
    success: bool
    error_message: str = ""

class KeywordOptimizer:
    """å…³é”®è¯ä¼˜åŒ–å™¨

    ä½¿ç”¨ç¡…åŸºæµåŠ¨çš„ Qwen3 æ¨¡å‹ï¼Œå°† Agent ç”Ÿæˆçš„æœç´¢è¯ä¼˜åŒ–ä¸ºæ›´è´´è¿‘
    ã€Œå°çº¢ä¹¦å“ç‰Œå†…å®¹/é€‰é¢˜åˆ†æã€åœºæ™¯çš„æ£€ç´¢å…³é”®è¯ã€‚
    """
    
    def __init__(self, api_key: str = None, base_url: str = None, model_name: str = None):
        """
        åˆå§‹åŒ–å…³é”®è¯ä¼˜åŒ–å™¨
        
        Args:
            api_key: ç¡…åŸºæµåŠ¨APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®æ–‡ä»¶è¯»å–
            base_url: æ¥å£åŸºç¡€åœ°å€ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶æä¾›çš„SiliconFlowåœ°å€
        """
        self.api_key = api_key or settings.KEYWORD_OPTIMIZER_API_KEY

        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ°ç¡…åŸºæµåŠ¨APIå¯†é’¥ï¼Œè¯·åœ¨config.pyä¸­è®¾ç½®KEYWORD_OPTIMIZER_API_KEY")

        self.base_url = base_url or settings.KEYWORD_OPTIMIZER_BASE_URL

        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        self.model = model_name or settings.KEYWORD_OPTIMIZER_MODEL_NAME
    
    def optimize_keywords(self, original_query: str, context: str = "") -> KeywordOptimizationResponse:
        """
        ä¼˜åŒ–æœç´¢å…³é”®è¯
        
        Args:
            original_query: Agentç”Ÿæˆçš„åŸå§‹æœç´¢æŸ¥è¯¢
            context: é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æ®µè½æ ‡é¢˜ã€å†…å®¹æè¿°ç­‰ï¼‰
            
        Returns:
            KeywordOptimizationResponse: ä¼˜åŒ–åçš„å…³é”®è¯åˆ—è¡¨
        """
        logger.info(f"ğŸ” å…³é”®è¯ä¼˜åŒ–ä¸­é—´ä»¶: å¤„ç†æŸ¥è¯¢ '{original_query}'")
        
        try:
            # æ„å»ºä¼˜åŒ–prompt
            system_prompt = self._build_system_prompt()
            user_prompt = self._build_user_prompt(original_query, context)
            
            # è°ƒç”¨Qwen API
            response = self._call_qwen_api(system_prompt, user_prompt)
            
            if response["success"]:
                # è§£æå“åº”
                content = response["content"]
                try:
                    # å°è¯•è§£æJSONæ ¼å¼çš„å“åº”
                    if content.strip().startswith('{'):
                        parsed = json.loads(content)
                        keywords = parsed.get("keywords", [])
                        reasoning = parsed.get("reasoning", "")
                    else:
                        # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯
                        keywords = self._extract_keywords_from_text(content)
                        reasoning = content
                    
                    # éªŒè¯å…³é”®è¯è´¨é‡
                    validated_keywords = self._validate_keywords(keywords)
                    
                    logger.info(
                        f"âœ… ä¼˜åŒ–æˆåŠŸ: {len(validated_keywords)}ä¸ªå…³é”®è¯" +
                        ("" if not validated_keywords else "\n" +
                         "\n".join([f"   {i}. '{k}'" for i, k in enumerate(validated_keywords, 1)]))
                    )
                        
                    
                    
                    return KeywordOptimizationResponse(
                        original_query=original_query,
                        optimized_keywords=validated_keywords,
                        reasoning=reasoning,
                        success=True
                    )
                
                except Exception as e:
                    logger.exception(f"âš ï¸ è§£æå“åº”å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {str(e)}")
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šä»åŸå§‹æŸ¥è¯¢ä¸­æå–å…³é”®è¯
                    fallback_keywords = self._fallback_keyword_extraction(original_query)
                    return KeywordOptimizationResponse(
                        original_query=original_query,
                        optimized_keywords=fallback_keywords,
                        reasoning="APIå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨å…³é”®è¯æå–",
                        success=True
                    )
            else:
                logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥: {response['error']}")
                # ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                fallback_keywords = self._fallback_keyword_extraction(original_query)
                return KeywordOptimizationResponse(
                    original_query=original_query,
                    optimized_keywords=fallback_keywords,
                    reasoning="APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨å…³é”®è¯æå–",
                    success=True,
                    error_message=response['error']
                )
                
        except Exception as e:
            logger.error(f"âŒ å…³é”®è¯ä¼˜åŒ–å¤±è´¥: {str(e)}")
            # æœ€ç»ˆå¤‡ç”¨æ–¹æ¡ˆ
            fallback_keywords = self._fallback_keyword_extraction(original_query)
            return KeywordOptimizationResponse(
                original_query=original_query,
                optimized_keywords=fallback_keywords,
                reasoning="ç³»ç»Ÿé”™è¯¯ï¼Œä½¿ç”¨å¤‡ç”¨å…³é”®è¯æå–",
                success=False,
                error_message=str(e)
            )
    
    def _build_system_prompt(self) -> str:
        """æ„å»ºç³»ç»Ÿ promptï¼ˆå“ç‰Œ/å°çº¢ä¹¦åœºæ™¯ï¼‰"""
        return """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ã€Œå°çº¢ä¹¦å“ç‰Œå†…å®¹ä¸é€‰é¢˜åˆ†æã€æ•°æ®æŒ–æ˜ä¸“å®¶ã€‚
ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·æä¾›çš„æœç´¢æŸ¥è¯¢ä¼˜åŒ–ä¸ºæ›´é€‚åˆåœ¨å°çº¢ä¹¦å“ç‰Œå†…å®¹æ•°æ®åº“ä¸­æŸ¥æ‰¾çš„å…³é”®è¯ã€‚

**æ ¸å¿ƒåŸåˆ™**ï¼š
1. **è´´è¿‘å°çº¢ä¹¦ç”¨æˆ·è¯­è¨€**ï¼šä½¿ç”¨æ™®é€šåšä¸»/ç”¨æˆ·åœ¨å°çº¢ä¹¦æ ‡é¢˜ã€æ ‡ç­¾ã€æœç´¢æ ä¸­ä¼šçœŸå®ä½¿ç”¨çš„è¯æ±‡
2. **é¿å…å®˜æ–¹/å­¦æœ¯æœ¯è¯­**ï¼šä¸ä½¿ç”¨ã€Œèˆ†æƒ…ã€ã€Œä¼ æ’­ã€ã€Œå€¾å‘ã€ã€Œå±•æœ›ã€ã€Œå‘å±•è¶‹åŠ¿ã€ç­‰ç ”ç©¶å‹è¯æ±‡
3. **ç®€æ´å…·ä½“**ï¼šæ¯ä¸ªå…³é”®è¯è¦ç®€æ´æ˜äº†ï¼Œä¾¿äºåœ¨æ ‡é¢˜/æ ‡ç­¾/æœç´¢ä¸­ç›´æ¥ä½¿ç”¨
4. **å›´ç»•å“ç‰Œ/å“ç±»åœºæ™¯**ï¼šç»“åˆå“ç‰Œå/äº§å“å + åœºæ™¯/åŠŸæ•ˆ/äººç¾¤/ä»·æ ¼å¸¦ç­‰ç»´åº¦
5. **æ•°é‡æ§åˆ¶**ï¼šæœ€å°‘æä¾› 8 ä¸ªå…³é”®è¯ï¼Œæœ€å¤šæä¾› 20 ä¸ªå…³é”®è¯
6. **é¿å…è·‘é¢˜**ï¼šä¸è¦è„±ç¦»åˆå§‹æŸ¥è¯¢çš„å“ç‰Œã€å“ç±»æˆ–æ ¸å¿ƒéœ€æ±‚

**é‡è¦æé†’**ï¼š
- æ¯ä¸ªå…³é”®è¯éƒ½å¿…é¡»æ˜¯ä¸€ä¸ªä¸å¯åˆ†å‰²çš„ç‹¬ç«‹è¯æ¡ï¼Œä¸¥ç¦åœ¨è¯æ¡å†…éƒ¨åŒ…å«ç©ºæ ¼ï¼›
- ä¾‹å¦‚ï¼Œåº”ä½¿ç”¨ã€Œé›ªèŠ±ç§€æ¶¦ç‡¥ç²¾åã€ã€Œç§‹å†¬å¹²çš®ç²‰åº•ã€è€Œä¸æ˜¯ã€Œé›ªèŠ±ç§€ æ¶¦ç‡¥ ç²¾åã€ã€‚

**è¾“å‡ºæ ¼å¼**ï¼š
è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼š
{
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"],
    "reasoning": "é€‰æ‹©è¿™äº›å…³é”®è¯çš„ç†ç”±ï¼ˆç”¨ä¸­æ–‡ç®€è¦è¯´æ˜ï¼‰"
}

**ç¤ºä¾‹ï¼ˆä»…ç¤ºæ„ï¼Œä¸è¦ç…§æ¬å…·ä½“è¯ï¼‰**ï¼š
è¾“å…¥ï¼š"é›ªèŠ±ç§€ç²¾å å­¦ç”Ÿå…š å¹²çš® é€‚åˆå—"
è¾“å‡ºï¼š
{
    "keywords": [
        "é›ªèŠ±ç§€ç²¾å",
        "é›ªèŠ±ç§€æ¶¦ç‡¥ç²¾å",
        "é›ªèŠ±ç§€ç²¾åå¹²çš®",
        "é›ªèŠ±ç§€ç²¾åå­¦ç”Ÿå…š",
        "å¹²çš®ç²¾åæ¨è",
        "å­¦ç”Ÿå…šæŠ¤è‚¤",
        "ç§‹å†¬å¹²çš®ç²¾å",
        "éŸ©ç³»ç²¾åæ¨è"
    ],
    "reasoning": "ä¼˜å…ˆä¿ç•™å“ç‰Œå’Œæ˜æ˜Ÿå•å“åç§°ï¼ŒåŒæ—¶åŠ å…¥å…¸å‹äººç¾¤ï¼ˆå­¦ç”Ÿå…šï¼‰ã€è‚¤è´¨ï¼ˆå¹²çš®ï¼‰ã€å­£èŠ‚ï¼ˆç§‹å†¬ï¼‰ç­‰ç»´åº¦ï¼Œä¾¿äºåœ¨å°çº¢ä¹¦ä¸­è¦†ç›–æ›´å¤šçœŸå®æœç´¢åœºæ™¯ã€‚"
}"""

    def _build_user_prompt(self, original_query: str, context: str) -> str:
        """æ„å»ºç”¨æˆ·prompt"""
        prompt = (
            "è¯·å°†ä»¥ä¸‹æœç´¢æŸ¥è¯¢ä¼˜åŒ–ä¸ºé€‚åˆ 'å°çº¢ä¹¦å“ç‰Œå†…å®¹/é€‰é¢˜åˆ†æ' åœºæ™¯çš„å…³é”®è¯ï¼š"
            f"\n\nåŸå§‹æŸ¥è¯¢ï¼š{original_query}"
        )
        
        if context:
            prompt += f"\n\nä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context}"
        
        prompt += "\n\nè¯·è®°ä½ï¼šè¦ä½¿ç”¨å°çº¢ä¹¦ç”¨æˆ·åœ¨æ ‡é¢˜/æ ‡ç­¾/æœç´¢ä¸­çœŸå®ä¼šç”¨çš„è¯æ±‡ï¼Œé¿å…å®˜æ–¹æœ¯è¯­å’Œå­¦æœ¯åŒ–è¡¨è¾¾ã€‚"
        
        return prompt
    
    @with_graceful_retry(SEARCH_API_RETRY_CONFIG, default_return={"success": False, "error": "å…³é”®è¯ä¼˜åŒ–æœåŠ¡æš‚æ—¶ä¸å¯ç”¨"})
    def _call_qwen_api(self, system_prompt: str, user_prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨Qwen API"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
            )

            if response.choices:
                content = response.choices[0].message.content
                return {"success": True, "content": content}
            else:
                return {"success": False, "error": "APIè¿”å›æ ¼å¼å¼‚å¸¸"}
        except Exception as e:
            return {"success": False, "error": f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}"}
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼ˆå½“JSONè§£æå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        # ç®€å•çš„å…³é”®è¯æå–é€»è¾‘
        lines = text.split('\n')
        keywords = []
        
        for line in lines:
            line = line.strip()
            # æŸ¥æ‰¾å¯èƒ½çš„å…³é”®è¯
            if 'ï¼š' in line or ':' in line:
                parts = line.split('ï¼š') if 'ï¼š' in line else line.split(':')
                if len(parts) > 1:
                    potential_keywords = parts[1].strip()
                    # å°è¯•åˆ†å‰²å…³é”®è¯
                    if 'ã€' in potential_keywords:
                        keywords.extend([k.strip() for k in potential_keywords.split('ã€')])
                    elif ',' in potential_keywords:
                        keywords.extend([k.strip() for k in potential_keywords.split(',')])
                    else:
                        keywords.append(potential_keywords)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
        if not keywords:
            # æŸ¥æ‰¾å¼•å·ä¸­çš„å†…å®¹
            import re
            quoted_content = re.findall(r'["""\'](.*?)["""\']', text)
            keywords.extend(quoted_content)
        
        # æ¸…ç†å’ŒéªŒè¯å…³é”®è¯
        cleaned_keywords = []
        for keyword in keywords[:20]:  # æœ€å¤š20ä¸ª
            keyword = keyword.strip().strip('"\'""''')
            if keyword and len(keyword) <= 20:  # åˆç†é•¿åº¦
                cleaned_keywords.append(keyword)
        
        return cleaned_keywords[:20]
    
    def _validate_keywords(self, keywords: List[str]) -> List[str]:
        """éªŒè¯å’Œæ¸…ç†å…³é”®è¯"""
        validated = []
        
        # ä¸è‰¯å…³é”®è¯ï¼ˆè¿‡äºä¸“ä¸šæˆ–å®˜æ–¹ï¼‰
        bad_keywords = {
            'æ€åº¦åˆ†æ', 'å…¬ä¼—ååº”', 'æƒ…ç»ªå€¾å‘',
            'æœªæ¥å±•æœ›', 'å‘å±•è¶‹åŠ¿', 'æˆ˜ç•¥è§„åˆ’', 'æ”¿ç­–å¯¼å‘', 'ç®¡ç†æœºåˆ¶'
        }
        
        for keyword in keywords:
            if isinstance(keyword, str):
                keyword = keyword.strip().strip('"\'""''')
                
                # åŸºæœ¬éªŒè¯
                if (keyword and 
                    len(keyword) <= 20 and 
                    len(keyword) >= 1 and
                    not any(bad_word in keyword for bad_word in bad_keywords)):
                    validated.append(keyword)
        
        return validated[:20]  # æœ€å¤šè¿”å›20ä¸ªå…³é”®è¯
    
    def _fallback_keyword_extraction(self, original_query: str) -> List[str]:
        """å¤‡ç”¨å…³é”®è¯æå–æ–¹æ¡ˆ"""
        # ç®€å•çš„å…³é”®è¯æå–é€»è¾‘
        # ç§»é™¤å¸¸è§çš„æ— ç”¨è¯æ±‡
        stop_words = {'ã€'}
        
        # åˆ†å‰²æŸ¥è¯¢
        import re
        # æŒ‰ç©ºæ ¼ã€æ ‡ç‚¹åˆ†å‰²
        tokens = re.split(r'[\sï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€]+', original_query)
        
        keywords = []
        for token in tokens:
            token = token.strip()
            if token and token not in stop_words and len(token) >= 2:
                keywords.append(token)
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢çš„ç¬¬ä¸€ä¸ªè¯
        if not keywords:
            first_word = original_query.split()[0] if original_query.split() else original_query
            keywords = [first_word] if first_word else ["çƒ­é—¨"]
        
        return keywords[:20]

# å…¨å±€å®ä¾‹
keyword_optimizer = KeywordOptimizer()
